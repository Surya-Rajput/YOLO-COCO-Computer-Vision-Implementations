{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mR0lO3q8dWj"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qUEnzb0yLsa"
      },
      "source": [
        "### HARRIS ALGORITHM\n",
        "This Python script performs corner detection on images using the Harris Corner Detection algorithm. The script defines a function called detectcorners(filename) that takes the filename of an image as input and proceeds with the following steps:\n",
        "\n",
        "Reads the image from the specified filename and converts the BGR channels to grayscale.\n",
        "Applies the Harris Corner Detection algorithm to detect corners in the image.\n",
        "Enhances the detected corners by dilating the output of the Harris Corner Detection.\n",
        "Creates a copy of the original image and marks the detected corners as red by drawing circles on the copied image.\n",
        "Returns the processed image with red circles drawn around the detected corners.\n",
        "The script then processes a list of images using the detectcorners() function, displaying the resulting images with highlighted corners. The user can press the 'Esc' key to close the displayed images.\n",
        "\n",
        "In summary, this code provides a simple demonstration of Harris Corner Detection, a technique widely used in computer vision and image processing to identify key points (corners) in images, which can be essential for various applications like feature matching, object recognition, and image stitching.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GXRGyVhtFjrL",
        "outputId": "361af832-abef-4a3a-99a4-db6988bb616e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def detectcorners(filename):\n",
        "    image = cv2.imread(filename) # reading the file\n",
        "    grayimage = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # converting BGR channels to grayscale\n",
        "    img = np.float32(grayimage) # algorithm requires the input image\n",
        "    dst = cv2.cornerHarris(img, 2, 3, 0.04) # taking blocksize 2x2, ksize = 3x3, and k = 0.04\n",
        "    dst = cv2.dilate(dst, None) # enhancing the detected corners\n",
        "\n",
        "    img_copy = image.copy()\n",
        "    img_copy[dst > 0.01 * dst.max()] = [0, 0, 255] # Marking corners as red using circles\n",
        "\n",
        "    # Draw circles at detected corners\n",
        "    corners = np.argwhere(dst > 0.01 * dst.max())\n",
        "    for corner in corners:\n",
        "        cv2.circle(img_copy, (corner[1], corner[0]), 1, (0, 0, 255), -1) # Circle with radius 1, color red, and filled\n",
        "\n",
        "    return img_copy\n",
        "\n",
        "filenames = ['chessboard.png', 'test.jpg', 'flower.png'] # passing in the images\n",
        "\n",
        "for filename in filenames:\n",
        "    processed_img = detectcorners(filename)\n",
        "    cv2_imshow(processed_img)\n",
        "    if cv2.waitKey(0) & 0xFF == 27:  # Press 'Esc' to close the window\n",
        "        break\n",
        "\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24JLg6xIy1wd"
      },
      "source": [
        "### IMAGE PYRAMID\n",
        "\n",
        "This Python code demonstrates the construction and display of image pyramids using the PyrDown and PyrUp functions from OpenCV. An image pyramid is a multi-scale representation of an image obtained by repeatedly down-sampling (PyrDown) or up-sampling (PyrUp) the original image. The purpose of image pyramids is to analyze and process images at multiple scales, enabling tasks like feature extraction, object detection, and image blending.\n",
        "\n",
        "Here's an overall summary of the code:\n",
        "\n",
        "The script defines three functions:\n",
        "a. create_image_pyramid_down(img, levels=4): This function generates an image pyramid by repeatedly down-sampling the input image img a specified number of levels (default is 4) using the cv2.pyrDown() function.\n",
        "b. create_image_pyramid_up(img, levels=4): This function creates an image pyramid by repeatedly up-sampling the input image img a specified number of levels (default is 4) using the cv2.pyrUp() function.\n",
        "c. display_image_pyramid(pyramid, title_prefix): This function displays the images in the image pyramid pyramid with window titles having a prefix specified by title_prefix.\n",
        "\n",
        "The code loads an image named \"flower.png\" using cv2.imread().\n",
        "\n",
        "It then creates two image pyramids using the create_image_pyramid_down() and create_image_pyramid_up() functions.\n",
        "\n",
        "The downsampled image pyramid is displayed using display_image_pyramid() with titles prefixed by 'Down'.\n",
        "\n",
        "The upsampled image pyramid is displayed using display_image_pyramid() with titles prefixed by 'Up'.\n",
        "\n",
        "The script waits for a key press and then closes all the displayed windows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Si95rlrsRU8V",
        "outputId": "a24ab4c7-3dae-4e0a-a819-bb612ec40ecc"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "# Defining a function to use pyrDown\n",
        "def create_image_pyramid_down(img, levels=4): #specifying levels=4 for downsize\n",
        "    pyramid_down = [img]\n",
        "    for i in range(levels):\n",
        "        img_downsampled = cv2.pyrDown(pyramid_down[i]) #using pyrDown function\n",
        "        pyramid_down.append(img_downsampled)\n",
        "    return pyramid_down\n",
        "\n",
        "def create_image_pyramid_up(img, levels=4):\n",
        "    pyramid_up = [img]\n",
        "    for i in range(levels):\n",
        "        img_upsampled = cv2.pyrUp(pyramid_up[i]) # using pyrUp function\n",
        "        pyramid_up.append(img_upsampled)\n",
        "    return pyramid_up\n",
        "\n",
        "def display_image_pyramid(pyramid, title_prefix): # displaying the image\n",
        "    for i, img in enumerate(pyramid):\n",
        "        cv2_imshow(img)\n",
        "\n",
        "# Load the image\n",
        "filename = 'flower.png'\n",
        "img = cv2.imread(filename)\n",
        "\n",
        "# Create and display the downsampled image pyramid\n",
        "pyramid_down = create_image_pyramid_down(img)\n",
        "display_image_pyramid(pyramid_down, 'Down')\n",
        "\n",
        "# Create and display the upsampled image pyramid\n",
        "pyramid_up = create_image_pyramid_up(img)\n",
        "display_image_pyramid(pyramid_up, 'Up')\n",
        "\n",
        "# Wait for a key press and then close all windows\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UifW9uUs4fqK"
      },
      "source": [
        "### SIFT ALGORITHM\n",
        "This Python code demonstrates the use of the SIFT (Scale-Invariant Feature Transform) feature detection algorithm to find keypoints and descriptors in an input image:\n",
        "\n",
        "The script loads an input image named \"flower.png\" using cv2.imread() and converts it to grayscale using cv2.cvtColor().\n",
        "\n",
        "It initializes the SIFT detector using cv2.SIFT_create().\n",
        "\n",
        "The SIFT detector is then used to find keypoints and descriptors in the grayscale image using the detectAndCompute() function.\n",
        "\n",
        "The code draws the detected keypoints on the original color image using cv2.drawKeypoints(). The keypoints are visualized as red circles with a size of 4 pixels.\n",
        "\n",
        "The image with keypoints is displayed using cv2_imshow().\n",
        "\n",
        "The script waits for a key press and then closes the displayed window using cv2.destroyAllWindows()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "rhifYwaTJESR",
        "outputId": "706df960-76cf-45d5-e3c7-afe394afccfb"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "# Load the input image\n",
        "filename = 'flower.png'\n",
        "img = cv2.imread(filename)\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Initialize the SIFT detector\n",
        "surf = cv2.SIFT_create()\n",
        "\n",
        "# Find keypoints and descriptors using SIFT\n",
        "keypoints, descriptors = surf.detectAndCompute(gray, None)\n",
        "\n",
        "# Draw keypoints on the image\n",
        "img_with_keypoints = cv2.drawKeypoints(img, keypoints, None, (0, 0, 255), 4)\n",
        "\n",
        "# Display the image with keypoints\n",
        "cv2_imshow(img_with_keypoints)\n",
        "\n",
        "# Wait for a key press and then close the window\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISim-kjU6cK1"
      },
      "source": [
        "### WATERSHED ALGORITHM\n",
        "his Python code performs image segmentation using the Watershed algorithm with marker-based region labeling. The code uses the OpenCV library for computer vision operations. Here's the summary of the code:\n",
        "\n",
        "The script reads an input image named \"coin.jpg\" using cv.imread() and converts it to grayscale using cv.cvtColor().\n",
        "\n",
        "It applies thresholding using the Otsu method to create a binary image thresh using cv.threshold().\n",
        "\n",
        "The code performs noise removal on the binary image using morphological opening with a 3x3 kernel and two iterations using cv.morphologyEx().\n",
        "\n",
        "It dilates the opened image to obtain the sure background area using cv.dilate().\n",
        "\n",
        "The distance transform is calculated on the opened image using cv.distanceTransform().\n",
        "\n",
        "The code applies thresholding to obtain the sure foreground area based on the distance transform.\n",
        "\n",
        "It subtracts the sure foreground from the sure background to find the unknown region.\n",
        "\n",
        "The markers are obtained using connected components labeling on the sure foreground.\n",
        "\n",
        "The markers are adjusted by adding one to all labels and marking the unknown region with zero.\n",
        "\n",
        "The Watershed algorithm is applied to the input image img with the markers obtained earlier using cv.watershed().\n",
        "\n",
        "The code assigns a red color (0, 0, 255) to the boundary regions marked as -1 in the Watershed result.\n",
        "\n",
        "The segmented image is displayed using cv2_imshow().\n",
        "\n",
        "The script waits for a key press and then closes the displayed window using cv2.waitKey(0) and cv2.destroyAllWindows()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "STiYrb1oAMXj",
        "outputId": "a132ba0e-7690-4942-9e7f-f0ff440c2547"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2 as cv\n",
        "from matplotlib import pyplot as plt\n",
        "img = cv.imread('coin.jpg')\n",
        "gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
        "ret, thresh = cv.threshold(gray,0,255,cv.THRESH_BINARY_INV+cv.THRESH_OTSU)\n",
        "\n",
        "\n",
        "# noise removal\n",
        "kernel = np.ones((3,3),np.uint8)\n",
        "opening = cv.morphologyEx(thresh,cv.MORPH_OPEN,kernel, iterations = 2)\n",
        "# sure background area\n",
        "sure_bg = cv.dilate(opening,kernel,iterations=3)\n",
        "# Finding sure foreground area\n",
        "dist_transform = cv.distanceTransform(opening,cv.DIST_L2,5)\n",
        "ret, sure_fg = cv.threshold(dist_transform,0.7*dist_transform.max(),255,0)\n",
        "# Finding unknown region\n",
        "sure_fg = np.uint8(sure_fg)\n",
        "unknown = cv.subtract(sure_bg,sure_fg)\n",
        "\n",
        "\n",
        "# Marker labelling\n",
        "ret, markers = cv.connectedComponents(sure_fg)\n",
        "# Add one to all labels so that sure background is not 0, but 1\n",
        "markers = markers+1\n",
        "# Now, mark the region of unknown with zero\n",
        "markers[unknown==255] = 0\n",
        "\n",
        "markers = cv.watershed(img,markers)\n",
        "img[markers == -1] = [0,0,255]\n",
        "\n",
        "# Display the result\n",
        "cv2_imshow(img)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HYJ4YBa8G7k"
      },
      "source": [
        "### ORB ALGORITHM\n",
        "This Python code demonstrates the use of the ORB (Oriented FAST and Rotated BRIEF) feature detection algorithm for finding keypoints and computing descriptors in a grayscale image. ORB is a fusion of FAST (Features from Accelerated Segment Test) and BRIEF (Binary Robust Independent Elementary Features), designed for real-time computer vision tasks.\n",
        "\n",
        "Here's the summary of the code:\n",
        "\n",
        "The script reads an input grayscale image named \"flower.png\" using cv.imread() with the flag cv.IMREAD_GRAYSCALE, which loads the image in grayscale.\n",
        "\n",
        "It initializes the ORB detector using cv.ORB_create().\n",
        "\n",
        "The code uses ORB to find keypoints in the grayscale image using orb.detect(). The keypoints are locations in the image where ORB has detected important features.\n",
        "\n",
        "It then computes the descriptors corresponding to the detected keypoints using orb.compute(). Descriptors represent the feature information around the keypoints, which are used for matching and recognition.\n",
        "\n",
        "The code draws green keypoints on the original grayscale image using cv.drawKeypoints(), displaying only the keypoints' locations without size and orientation information.\n",
        "\n",
        "The modified image with keypoints is displayed using plt.imshow() and plt.show() from the matplotlib library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "vpbZPbqmGTYz",
        "outputId": "5ec5a849-047a-4c51-ac27-ecdc9d5d8291"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2 as cv\n",
        "from matplotlib import pyplot as plt\n",
        "img = cv.imread('flower.png', cv.IMREAD_GRAYSCALE)\n",
        "# Initiate ORB detector\n",
        "orb = cv.ORB_create()\n",
        "# find the keypoints with ORB\n",
        "kp = orb.detect(img,None)\n",
        "# compute the descriptors with ORB\n",
        "kp, des = orb.compute(img, kp)\n",
        "# draw only keypoints location,not size and orientation\n",
        "img2 = cv.drawKeypoints(img, kp, None, color=(0,255,0), flags=0)\n",
        "plt.imshow(img2), plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiKfwPd59JZC"
      },
      "source": [
        "### YOLO ON COCO DATASET\n",
        "The code defines a function named detect_and_draw_objects_custom that takes an image file path as input.\n",
        "\n",
        "The function reads the input image from the specified path using OpenCV's cv2.imread function.\n",
        "\n",
        "It loads the names of classes from the 'coco.names' file and generates random colors for each class using NumPy.\n",
        "\n",
        "The YOLOv3 model is loaded using OpenCV's cv2.dnn.readNetDarknet function, and the backend is set to use the OpenCV backend.\n",
        "\n",
        "The output layer names of the YOLOv3 model are obtained using net.getLayerNames, and the corresponding indices are retrieved using net.getUnconnectedOutLayers.\n",
        "\n",
        "The output layer indices are converted to layer names, and the resulting list of output layer names is stored in output_layers.\n",
        "\n",
        "The input image is preprocessed into a blob using cv2.dnn.blobFromImage. The blob is then set as the input to the YOLOv3 model using net.setInput, and the forward method is called to perform inference. The output detections are stored in the outputs variable.\n",
        "\n",
        "The function initializes lists (boxes, confidences, class_ids) to store information about detected objects.\n",
        "\n",
        "The code iterates through each output in outputs and further iterates through each detection in the output.\n",
        "\n",
        "For each detection, the class with the highest confidence score is determined, and the confidence score is checked to be greater than 0.5.\n",
        "\n",
        "If the confidence score is above the threshold, the bounding box coordinates are calculated based on the detection results and the size of the input image. The bounding box coordinates are then appended to the boxes list, the confidence score to the confidences list, and the class ID to the class_ids list.\n",
        "\n",
        "After processing all detections, Non-Maximum Suppression (NMS) is applied using cv2.dnn.NMSBoxes to remove overlapping bounding boxes. The function is provided with the boxes, confidences, and NMS threshold values (0.5 and 0.4).\n",
        "\n",
        "The function returns the indices of the selected bounding boxes after NMS, which represent the detected objects in the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFAko6N-p6DI",
        "outputId": "25137841-fb60-40ad-ca05-ade69929bb22"
      },
      "outputs": [],
      "source": [
        "# Download the YOLOv3 configuration and weights\n",
        "!wget https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg\n",
        "!wget https://pjreddie.com/media/files/yolov3.weights\n",
        "\n",
        "# Download the COCO dataset class names\n",
        "!wget https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1Fn7DrvtFub",
        "outputId": "fb6569e2-439c-4008-9175-9cb8243335bb"
      },
      "outputs": [],
      "source": [
        "!cat coco.names | wc -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ma3X7wkuxjcM",
        "outputId": "8611830e-e4ad-4922-95ad-1ce0971de6ea"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def detect_and_draw_objects_custom(image_path):\n",
        "    # Read the image from the specified path.\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Load names of classes and generate random colors for each class.\n",
        "    with open('coco.names', 'r') as class_file:\n",
        "        classes = class_file.read().strip().split('\\n')\n",
        "    np.random.seed(42)\n",
        "    colors = np.random.randint(0, 255, size=(len(classes), 3), dtype='uint8')\n",
        "\n",
        "    # Load YOLOv3 model and set it to use OpenCV backend.\n",
        "    net = cv2.dnn.readNetFromDarknet('yolov3.cfg', 'yolov3.weights')\n",
        "    net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
        "\n",
        "    # Get output layer names.\n",
        "    layer_names = net.getLayerNames()\n",
        "    output_layer_indices = net.getUnconnectedOutLayers()\n",
        "\n",
        "    # Convert output layer indices to layer names.\n",
        "    output_layers = [layer_names[i - 1] for i in output_layer_indices]\n",
        "\n",
        "    # Preprocess the image and get the detections.\n",
        "    blob = cv2.dnn.blobFromImage(image, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
        "    net.setInput(blob)\n",
        "    outputs = net.forward(output_layers)\n",
        "\n",
        "    # Process the detections and draw bounding boxes around objects.\n",
        "    boxes = []\n",
        "    confidences = []\n",
        "    class_ids = []\n",
        "    h, w = image.shape[:2]\n",
        "\n",
        "    for output in outputs:\n",
        "        for detection in output:\n",
        "            scores = detection[5:]\n",
        "            class_id = np.argmax(scores)\n",
        "            confidence = scores[class_id]\n",
        "            if confidence > 0.5:\n",
        "                box = detection[:4] * np.array([w, h, w, h])\n",
        "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
        "                x = int(centerX - (width / 2))\n",
        "                y = int(centerY - (height / 2))\n",
        "                box = [x, y, int(width), int(height)]\n",
        "                boxes.append(box)\n",
        "                confidences.append(float(confidence))\n",
        "                class_ids.append(class_id)\n",
        "\n",
        "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
        "\n",
        "    # Draw bounding boxes and labels for the detected objects.\n",
        "    if len(indices) > 0:\n",
        "        for i in indices.flatten():\n",
        "            (x, y) = (boxes[i][0], boxes[i][1])\n",
        "            (w, h) = (boxes[i][2], boxes[i][3])\n",
        "            color = [int(c) for c in colors[class_ids[i]]]\n",
        "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 4)\n",
        "            text = \"{}: {:.4f}\".format(classes[class_ids[i]], confidences[i])\n",
        "            cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
        "\n",
        "    return image\n",
        "\n",
        "# Process 5 images and display the results using Matplotlib.\n",
        "img_paths = ['dog.jpg','dogbike.jpg','elephant.jpg','peopletest.jpg','city.jpg']\n",
        "\n",
        "for img_path in img_paths:\n",
        "    processed_img = detect_and_draw_objects_custom(img_path)\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(cv2.cvtColor(processed_img, cv2.COLOR_BGR2RGB))\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
